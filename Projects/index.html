layout: raw-page

I am always on the lookout for strong PhD students interested interested to work on a variety of topics in Deep Learning. <br><br>


Currently I am particularly recruiting motivated PhD, interns, and Masterâ€™s students to work on projects in distributed deep learning methods and federated learning. 
The over-arching goal of this research is to develop scalable algorithms that can unlock existing challenges in scaling up large deep learning systems. 
Students should have strong programming skills and familiarity with deep learning methods, and demonstrate genuine interest in the research topic. 
If contacting about this opportunity put [ReadYourSite] in your email title to let me know you read this page. <br><br>


Below are some related publications to give an idea of the research projects<br>

<b> Model-Parallelism <b>
- Laskin et al, Parallel Training of Deep Networks with Local Updates ICLR 2020<br>
- Belilovsky, Eugene, Michael Eickenberg, and Edouard Oyallon. "Decoupled greedy learning of cnns." ICML 2020.<br>
- Ehsan Amid, Rohan Anil, Manfred K. Warmuth LocoProp: Enhancing BackProp via Local Loss Optimization https://arxiv.org/pdf/2106.06199.pdf<br>
- Choromanska, Anna, et al. "Beyond backprop: Online alternating minimization with auxiliary variables." ICML 2019.<br>
- Nokland et al, Direct Feedback Alignment Provides Learning in Deep Neural Networks NeurIPS 2016<br>
- Xu, An, Zhouyuan Huo, and Heng Huang. "On the Acceleration of Deep Learning Model Parallelism With Staleness." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.<br>

** Decentralized Learning **<br>

- Towards Crowdsourced Training of Large Neural Networks using Decentralized Mixture-of-Experts (https://arxiv.org/pdf/2002.04013.pdf)<br>


** Federated Learning ** <br>

- https://arxiv.org/pdf/1602.05629.pdf<br>
- https://arxiv.org/pdf/2106.06639.pdf<br>
- https://arxiv.org/abs/2109.04833<br>

  
